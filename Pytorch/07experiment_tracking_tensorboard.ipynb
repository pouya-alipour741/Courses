{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa1a64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu118 0.15.2+cu118\n",
      "imported torch version: 2.0.1+cu118, torchvision version 0.15.2+cu118\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) > 13\n",
    "    print(torch.__version__, torchvision.__version__)\n",
    "    print(f\"imported torch version: {torch.__version__}, torchvision version {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda10cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b992e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a2d95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pizza_steak_sushi.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\").name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7311e9a",
   "metadata": {},
   "source": [
    "### get data | test\n",
    "----| ----|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd9eef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "source = Path(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "target_file = Path(source).name\n",
    "download_path = data_path / target_file\n",
    "download_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e42981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests, zipfile, os\n",
    "\n",
    "###modified version\n",
    "def download_data(source: str,\n",
    "                 destination: str,\n",
    "                 remove_source: bool = True) ->Path:\n",
    "    \n",
    "    data_path = Path(\"data\")\n",
    "    image_path = data_path / destination\n",
    "    target_file = Path(source).name\n",
    "    download_path = data_path / target_file\n",
    "    \n",
    "    if image_path.is_dir():\n",
    "        print(f\"file already exist\")\n",
    "    elif not download_path.is_file():\n",
    "        image_path.mkdir(parents=True, exist_ok=True)        \n",
    "        with open(data_path / target_file, \"wb\") as f:\n",
    "            request = requests.get(source)\n",
    "            print(f\"Downloading {target_file} from {source}...\")\n",
    "            f.write(request.content)\n",
    "\n",
    "        print(f\"extracting {target_file} to {image_path}\")\n",
    "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(image_path)\n",
    "            \n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "    else:\n",
    "        print(f\"extracting {target_file} to {image_path}\")\n",
    "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(image_path)\n",
    "            \n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "            \n",
    "    return image_path\n",
    "\n",
    "\n",
    "download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "             destination=\"pizza_steak_sushi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a221c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ed8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_transforms: ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x2bc8829cd60>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x2bc8829c520>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular.going_modular import data_setup, engine\n",
    "\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "BATCH_SIZE = 32\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "auto_transform = weights.transforms()\n",
    "print(f\"auto_transforms: {auto_transform}\")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir,\n",
    "                                                                                test_dir,\n",
    "                                                                                auto_transform,\n",
    "                                                                                BATCH_SIZE)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e026c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed48102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134c8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "                                 nn.Dropout(p=0.8,inplace=True),\n",
    "                                 nn.Linear(in_features=1280,\n",
    "                                          out_features=len(class_names),\n",
    "                                          bias=True)\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d273762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape     Output Shape    Param #         Trainable\n",
       "========================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 244] [32, 3]         --              Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 244] [32, 1280, 7, 8] --              False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 244] [32, 32, 112, 122] --              False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 244] [32, 32, 112, 122] (864)           False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 122] [32, 32, 112, 122] (64)            False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 122] [32, 32, 112, 122] --              --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 122] [32, 16, 112, 122] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 122] [32, 16, 112, 122] (1,448)         False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 122] [32, 24, 56, 61] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 122] [32, 24, 56, 61] (6,004)         False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 61] [32, 24, 56, 61] (10,710)        False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 61] [32, 40, 28, 31] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 61] [32, 40, 28, 31] (15,350)        False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 31] [32, 40, 28, 31] (31,290)        False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 31] [32, 80, 14, 16] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 31] [32, 80, 14, 16] (37,130)        False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 16] [32, 80, 14, 16] (102,900)       False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 16] [32, 80, 14, 16] (102,900)       False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 16] [32, 112, 14, 16] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 16] [32, 112, 14, 16] (126,004)       False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 16] [32, 112, 14, 16] (208,572)       False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 16] [32, 112, 14, 16] (208,572)       False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 16] [32, 192, 7, 8] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 16] [32, 192, 7, 8] (262,492)       False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 8] [32, 192, 7, 8] (587,952)       False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 8] [32, 192, 7, 8] (587,952)       False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 8] [32, 192, 7, 8] (587,952)       False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 8] [32, 320, 7, 8] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 8] [32, 320, 7, 8] (717,232)       False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 8] [32, 1280, 7, 8] --              False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 8] [32, 1280, 7, 8] (409,600)       False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 8] [32, 1280, 7, 8] (2,560)         False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 8] [32, 1280, 7, 8] --              --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 8] [32, 1280, 1, 1] --              --\n",
       "├─Sequential (classifier)                                    [32, 1280]      [32, 3]         --              True\n",
       "│    └─Dropout (0)                                           [32, 1280]      [32, 1280]      --              --\n",
       "│    └─Linear (1)                                            [32, 1280]      [32, 3]         3,843           True\n",
       "========================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 13.87\n",
       "========================================================================================================================\n",
       "Input size (MB): 20.99\n",
       "Forward/backward pass size (MB): 3821.27\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3858.31\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model,\n",
    "   input_size=(32,3,224,244),\n",
    "       col_names = [\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "       col_width=15,\n",
    "       row_settings=[\"var_names\"],\n",
    "       verbose= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e266ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "220393f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ModuleNotFoundError:\n",
    "    !pip install tensorboard\n",
    "    from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd928ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# (data_path/\"runs\").mkdir(parents=True,exist_ok=True)\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157b18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from going_modular.going_modular.engine import train_step,test_step\n",
    "# from tqdm.auto import tqdm\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def train(model,\n",
    "#          train_data_loader,\n",
    "#           test_data_loader,\n",
    "#          loss_fn,\n",
    "#          optimizer,\n",
    "#          device,\n",
    "#          epochs = 5):\n",
    "    \n",
    "#     results = defaultdict(list)\n",
    "    \n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         train_loss, train_acc = train_step(model,\n",
    "#                   train_dataloader,\n",
    "#                   loss_fn,\n",
    "#                   optimizer,\n",
    "#                   device)\n",
    "\n",
    "#         test_loss,test_acc  = test_step(model,\n",
    "#                  test_dataloader,\n",
    "#                  loss_fn,\n",
    "#                  device)\n",
    "        \n",
    "#         print(\n",
    "#           f\"Epoch: {epoch+1} | \"\n",
    "#           f\"train_loss: {train_loss:.4f} | \"\n",
    "#           f\"train_acc: {train_acc:.4f} | \"\n",
    "#           f\"test_loss: {test_loss:.4f} | \"\n",
    "#           f\"test_acc: {test_acc:.4f}\"\n",
    "#         )\n",
    "        \n",
    "#         results[\"train_loss\"].append(train_loss)\n",
    "#         results[\"train_acc\"].append(train_acc)\n",
    "#         results[\"test_loss\"].append(test_loss)\n",
    "#         results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "#         ##Experiment tracking\n",
    "#         writer.add_scalars(main_tag=\"Loss\",\n",
    "#                         tag_scalar_dict={\"train_loss\":train_loss,\"test_loss\":test_loss},\n",
    "#                           global_step=epoch) \n",
    "        \n",
    "#         writer.add_scalars(main_tag=\"Accuracy\",\n",
    "#                           tag_scalar_dict={\"train_acc\":train_acc,\"test_acc\":test_acc},\n",
    "#                           global_step=epoch)\n",
    "        \n",
    "#         writer.add_graph(model=model,\n",
    "#                          ## pass in an example input\n",
    "#                         input_to_model=torch.randn(32,3,224,224).to(device))\n",
    "        \n",
    "#     writer.close()\n",
    "    \n",
    "#     return results\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e8d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed()\n",
    "# results = train(model,\n",
    "#                train_dataloader,\n",
    "#                test_dataloader,\n",
    "#                loss_fn,\n",
    "#                optimizer,\n",
    "#                device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "298f7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "##practice\n",
    "# from going_modular.going_modular import engine_with_tensorboard\n",
    "\n",
    "# results = engine_with_tensorboard.train(model,\n",
    "#                               train_dataloader,\n",
    "#                               test_dataloader,\n",
    "#                               loss_fn,\n",
    "#                               optimizer,\n",
    "#                               device,\n",
    "#                               epochs=5\n",
    "#                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5c96d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "956a37d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\pouya\\\\jupyter projects\\\\PyTorch_for Deep Learning_Machine Learning',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\python39.zip',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\DLLs',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\pouya\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path\n",
    "# sys.executable\n",
    "# !pip3 show tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcda9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4e3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7ed2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_writer(experiment_name,\n",
    "#                  model_name,\n",
    "#                  extra=None):\n",
    "#     \"\"\" create a custom writer\n",
    "    \n",
    "#     Args:\n",
    "#         experiment name\n",
    "#         model\n",
    "#         extra\n",
    "        \n",
    "#     Returns:\n",
    "#         torch.utils.tensorboard.writer.SummaryWriter()\n",
    "        \n",
    "#     \"\"\"\n",
    "    \n",
    "#     from datetime import datetime as dt\n",
    "#     import os\n",
    "#     from pathlib import Path\n",
    "    \n",
    "# #     now = dt.now().strftime(\"%Y-%m-%d\")\n",
    "#     now = dt.now().strftime(\"%b %d-%Y\")\n",
    "    \n",
    "#     if extra:\n",
    "# #         log_dir = os.path.join(\"runs\",now, experiment_name, model_name, extra)\n",
    "#         log_dir = Path(\"runs\")/now/ experiment_name/ model_name/ extra\n",
    "        \n",
    "#     else:\n",
    "# #         log_dir = os.path.join(\"runs\",now, experiment_name, model_name, extra)\n",
    "#         log_dir = Path(\"runs\")/now/ experiment_name/ model_name\n",
    "        \n",
    "#     print(f\"saving log to {log_dir}\")\n",
    "#     return SummaryWriter(log_dir=log_dir)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e981440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_writer(\"data_10%\",\n",
    "#              \"efficientnetb0\",\n",
    "#              \"5_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7936686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f41792ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular.engine import train_step, test_step\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(model,\n",
    "         train_dataloader,\n",
    "         test_loader,\n",
    "         optimizer,\n",
    "         loss_fn,\n",
    "         device,\n",
    "         epochs,\n",
    "         writer,\n",
    "         ):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "              \"test_loss\":[],\n",
    "              \"train_acc\":[],\n",
    "              \"test_acc\":[]}\n",
    "    \n",
    "    train_start_time = perf_counter()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model,\n",
    "                                          train_dataloader,\n",
    "                                          loss_fn,\n",
    "                                          optimizer,\n",
    "                                          device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model,\n",
    "                                       test_dataloader,\n",
    "                                       loss_fn,\n",
    "                                       device\n",
    "                                       )\n",
    "        \n",
    "        print( f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"       \n",
    "        )\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag=\"loss\",\n",
    "                              tag_scalar_dict={\"train_loss\":train_loss,\n",
    "                                              \"test_loss\":test_loss},\n",
    "                              global_step=epoch)\n",
    "            \n",
    "            writer.add_scalars(main_tag=\"accuracy\",\n",
    "                              tag_scalar_dict={\"train_acc\":train_acc,\n",
    "                                              \"test_acc\":test_acc},\n",
    "                              global_step=epoch)\n",
    "            \n",
    "                \n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    train_end_time = perf_counter()   \n",
    "    \n",
    "    print(f\"total train time took {train_end_time-train_start_time} on {device}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3de29466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed()\n",
    "\n",
    "# writer = create_writer(\"data_10%_customWriter\",\n",
    "#              \"efficientnetb0\",\n",
    "#              \"4_epochs\")\n",
    "# results = train(model,\n",
    "#                train_dataloader,\n",
    "#                test_dataloader,     \n",
    "#                optimizer,\n",
    "#                 loss_fn,\n",
    "#                device,\n",
    "#                 4,\n",
    "#                 writer,\n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce091175",
   "metadata": {},
   "source": [
    "Breaking these down we get: \n",
    "\n",
    "| Experiment number | Training Dataset | Model (pretrained on ImageNet) | Number of epochs |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 5 |\n",
    "| 2 | Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 5 | \n",
    "| 3 | Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 10 | \n",
    "| 4 | Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 10 |\n",
    "| 5 | Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 5 |\n",
    "| 6 | Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 5 |\n",
    "| 7 | Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 10 |\n",
    "| 8 | Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 10 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14536777",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create download data function again\n",
    "\n",
    "def download_data(source,\n",
    "                 destination,\n",
    "                  remove_source=True\n",
    "                 ):\n",
    "    \n",
    "    data_path = Path(\"data\")\n",
    "    image_path = data_path / destination\n",
    "    \n",
    "    if image_path.is_dir():\n",
    "        print(\"already exist\")\n",
    "    else:\n",
    "        print(f\"{image_path} does not exist,downloading...\")\n",
    "        target_dir = Path(source).name\n",
    "        image_path.mkdir(parents=True,exist_ok=True)\n",
    "        with open(data_path/target_dir, \"wb\") as f:\n",
    "            response = requests.get(source)\n",
    "            f.write(response.content)\n",
    "            \n",
    "        with zipfile.ZipFile(data_path/target_dir, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(image_path)\n",
    "            \n",
    "        if remove_source:\n",
    "            os.remove(data_path/target_dir)\n",
    "            \n",
    "    return image_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70f2006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exist\n",
      "already exist\n"
     ]
    }
   ],
   "source": [
    "data_10_percent_path = download_data(source= r\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "             destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=r\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                    destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "356186fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi_20_percent/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "##we keep test dir same for both of our tests\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "train_dir_10_percent,train_dir_20_percent,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "773fc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b7d705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_10_percent = datasets.ImageFolder(root=train_dir_10_percent,\n",
    "                                            transform=simple_transform)\n",
    "\n",
    "train_data_20_percent = datasets.ImageFolder(train_dir_20_percent,\n",
    "                                            simple_transform)\n",
    "\n",
    "test_data_10_percent = datasets.ImageFolder(test_dir,\n",
    "                                           simple_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "355208af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 8\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)\n",
      "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader_10_percent = DataLoader(train_data_10_percent,\n",
    "                                        BATCH_SIZE,\n",
    "                                        True,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "train_dataloader_20_percent = DataLoader(train_data_20_percent,\n",
    "                                        BATCH_SIZE,\n",
    "                                        True,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "test_dataloader_10_percent = DataLoader(test_data_10_percent,\n",
    "                                        BATCH_SIZE,\n",
    "                                        True,\n",
    "                                       pin_memory=True)\n",
    "\n",
    "class_names = train_data_10_percent.classes\n",
    "\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0aac60e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape     Output Shape    Param #         Trainable\n",
       "========================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224] [32, 1000]      --              True\n",
       "├─Sequential (features)                                      [32, 3, 224, 224] [32, 1280, 7, 7] --              True\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224] [32, 32, 112, 112] --              True\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224] [32, 32, 112, 112] 864             True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112] [32, 32, 112, 112] 64              True\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112] [32, 32, 112, 112] --              --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112] [32, 16, 112, 112] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112] [32, 16, 112, 112] 1,448           True\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112] [32, 24, 56, 56] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112] [32, 24, 56, 56] 6,004           True\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56] [32, 24, 56, 56] 10,710          True\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56] [32, 40, 28, 28] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56] [32, 40, 28, 28] 15,350          True\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28] [32, 40, 28, 28] 31,290          True\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28] [32, 80, 14, 14] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28] [32, 80, 14, 14] 37,130          True\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14] [32, 80, 14, 14] 102,900         True\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14] [32, 80, 14, 14] 102,900         True\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14] [32, 112, 14, 14] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14] [32, 112, 14, 14] 126,004         True\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14] [32, 112, 14, 14] 208,572         True\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14] [32, 112, 14, 14] 208,572         True\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14] [32, 192, 7, 7] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14] [32, 192, 7, 7] 262,492         True\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7] [32, 192, 7, 7] 587,952         True\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7] [32, 192, 7, 7] 587,952         True\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7] [32, 192, 7, 7] 587,952         True\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7] [32, 320, 7, 7] --              True\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7] [32, 320, 7, 7] 717,232         True\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7] [32, 1280, 7, 7] --              True\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7] [32, 1280, 7, 7] 409,600         True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7] [32, 1280, 7, 7] 2,560           True\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7] [32, 1280, 7, 7] --              --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7] [32, 1280, 1, 1] --              --\n",
       "├─Sequential (classifier)                                    [32, 1280]      [32, 1000]      --              True\n",
       "│    └─Dropout (0)                                           [32, 1280]      [32, 1280]      --              --\n",
       "│    └─Linear (1)                                            [32, 1280]      [32, 1000]      1,281,000       True\n",
       "========================================================================================================================\n",
       "Total params: 5,288,548\n",
       "Trainable params: 5,288,548\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.35\n",
       "========================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.35\n",
       "Params size (MB): 21.15\n",
       "Estimated Total Size (MB): 3492.77\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "effnetb2_model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "summary(effnetb2_model,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "       col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "       col_width=15,\n",
    "       row_settings=[\"var_names\"] \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acbdc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "def create_effnetb0():\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "    \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    set_seed()\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "    \n",
    "    model.name = \"effnetb0\"\n",
    "    \n",
    "    print(f\"created {model.name} model\")\n",
    "    \n",
    "    return model   \n",
    "\n",
    "def create_effnetb2():\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seed()\n",
    "\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"Created {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e412ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(model):\n",
    "    report = summary(model,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "       col_names=[\"input_size\",\"output_size\",\"num_params\",\"trainable\"],\n",
    "       col_width=15,\n",
    "       row_settings=[\"var_names\"] \n",
    "       )\n",
    "\n",
    "            \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bb7af57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created effnetb0 model\n",
      "Created effnetb2 model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape     Output Shape    Param #         Trainable\n",
       "========================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224] [32, 3]         --              Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224] [32, 1408, 7, 7] --              False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224] [32, 32, 112, 112] --              False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224] [32, 32, 112, 112] (864)           False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112] [32, 32, 112, 112] (64)            False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112] [32, 32, 112, 112] --              --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112] [32, 16, 112, 112] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112] [32, 16, 112, 112] (1,448)         False\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112] [32, 16, 112, 112] (612)           False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112] [32, 24, 56, 56] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112] [32, 24, 56, 56] (6,004)         False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56] [32, 24, 56, 56] (10,710)        False\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56] [32, 24, 56, 56] (10,710)        False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56] [32, 48, 28, 28] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56] [32, 48, 28, 28] (16,518)        False\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28] [32, 48, 28, 28] (43,308)        False\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28] [32, 48, 28, 28] (43,308)        False\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28] [32, 88, 14, 14] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28] [32, 88, 14, 14] (50,300)        False\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14] [32, 88, 14, 14] (123,750)       False\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14] [32, 88, 14, 14] (123,750)       False\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14] [32, 88, 14, 14] (123,750)       False\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14] [32, 120, 14, 14] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14] [32, 120, 14, 14] (149,158)       False\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14] [32, 120, 14, 14] (237,870)       False\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14] [32, 120, 14, 14] (237,870)       False\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14] [32, 120, 14, 14] (237,870)       False\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14] [32, 208, 7, 7] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14] [32, 208, 7, 7] (301,406)       False\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7] [32, 208, 7, 7] (686,868)       False\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7] [32, 208, 7, 7] (686,868)       False\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7] [32, 208, 7, 7] (686,868)       False\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7] [32, 208, 7, 7] (686,868)       False\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7] [32, 352, 7, 7] --              False\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7] [32, 352, 7, 7] (846,900)       False\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7] [32, 352, 7, 7] (1,888,920)     False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7] [32, 1408, 7, 7] --              False\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7] [32, 1408, 7, 7] (495,616)       False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7] [32, 1408, 7, 7] (2,816)         False\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7] [32, 1408, 7, 7] --              --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7] [32, 1408, 1, 1] --              --\n",
       "├─Sequential (classifier)                                    [32, 1408]      [32, 3]         --              True\n",
       "│    └─Dropout (0)                                           [32, 1408]      [32, 1408]      --              --\n",
       "│    └─Linear (1)                                            [32, 1408]      [32, 3]         4,227           True\n",
       "========================================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (G): 21.04\n",
       "========================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.53\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 5067.62\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb0 = create_effnetb0()\n",
    "effnetb2 = create_effnetb2()\n",
    "\n",
    "get_summary(effnetb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83a8a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(Path(\"runs2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "389e5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular.engine import train_step, test_step\n",
    "from time import perf_counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(model,\n",
    "         train_dataloader,\n",
    "         test_loader,\n",
    "         optimizer,\n",
    "         loss_fn,\n",
    "         device,\n",
    "         epochs,\n",
    "         writer,\n",
    "          step,\n",
    "         ):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "              \"test_loss\":[],\n",
    "              \"train_acc\":[],\n",
    "              \"test_acc\":[]}\n",
    "    \n",
    "    train_start_time = perf_counter()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model,\n",
    "                                          train_dataloader,\n",
    "                                          loss_fn,\n",
    "                                          optimizer,\n",
    "                                          device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model,\n",
    "                                       test_dataloader,\n",
    "                                       loss_fn,\n",
    "                                       device\n",
    "                                       )\n",
    "        \n",
    "        print( f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"       \n",
    "        )\n",
    "        \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        if writer:\n",
    "            writer.add_scalar(tag=\"training_loss\",\n",
    "                             scalar_value=train_loss,\n",
    "                             global_step=step)\n",
    "            \n",
    "            writer.add_scalar(\"training accuracy\",\n",
    "                              train_acc,\n",
    "                             global_step=step)\n",
    "                            \n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    train_end_time = perf_counter()   \n",
    "    \n",
    "    print(f\"total train time took {train_end_time-train_start_time} on {device}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a5724ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = [5,10]\n",
    "\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "\n",
    "train_dataloaders = {\"data_10_percent\":train_dataloader_10_percent,\n",
    "                    \"data_20_percent\":train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25ad659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##more practice\n",
    "# def create_writer(experiment_name,\n",
    "#                  model_name,\n",
    "#                  extra=None):\n",
    "    \n",
    "#     now = dt.now().strftime(\"%d-%m-%Y\")\n",
    "#     if extra:\n",
    "#         log_dir = Path(\"runs\")/now /experiment_name/ model_name/extra\n",
    "#     else:\n",
    "#         log_dir = Path(\"runs\")/now /experiment_name/ model_name\n",
    "    \n",
    "#     print(f\"saving to {log_dir}\")\n",
    "#     SummaryWriter(log_dir=log_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64b03010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d479047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment number: 1\n",
      "number of epochs: 5\n",
      "model: effnetb0\n",
      "data_loader:data_10_percent\n",
      "created effnetb0 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f5dc69b24f4a2dbed7f9260564f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0520 | train_acc: 0.4961 | test_loss: 0.8505 | test_acc: 0.6496\n",
      "Epoch: 2 | train_loss: 0.9251 | train_acc: 0.6055 | test_loss: 0.7662 | test_acc: 0.7017\n",
      "Epoch: 3 | train_loss: 0.7707 | train_acc: 0.6992 | test_loss: 0.6602 | test_acc: 0.8759\n",
      "Epoch: 4 | train_loss: 0.7085 | train_acc: 0.7617 | test_loss: 0.6719 | test_acc: 0.8059\n",
      "Epoch: 5 | train_loss: 0.7050 | train_acc: 0.7773 | test_loss: 0.6078 | test_acc: 0.8759\n",
      "total train time took 17.455327399999987 on cuda\n",
      "experiment number: 2\n",
      "number of epochs: 5\n",
      "model: effnetb0\n",
      "data_loader:data_20_percent\n",
      "created effnetb0 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0866016ad06d4294a92a9595ef6949ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9588 | train_acc: 0.6125 | test_loss: 0.6506 | test_acc: 0.8759\n",
      "Epoch: 2 | train_loss: 0.6833 | train_acc: 0.8438 | test_loss: 0.5657 | test_acc: 0.9072\n",
      "Epoch: 3 | train_loss: 0.5729 | train_acc: 0.8792 | test_loss: 0.4881 | test_acc: 0.9072\n",
      "Epoch: 4 | train_loss: 0.5320 | train_acc: 0.8271 | test_loss: 0.4978 | test_acc: 0.8570\n",
      "Epoch: 5 | train_loss: 0.4486 | train_acc: 0.8833 | test_loss: 0.4077 | test_acc: 0.9176\n",
      "total train time took 30.10120460000053 on cuda\n",
      "experiment number: 3\n",
      "number of epochs: 5\n",
      "model: effnetb2\n",
      "data_loader:data_10_percent\n",
      "Created effnetb2 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730b17ef506c46b2ac8005f1954128d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0914 | train_acc: 0.3867 | test_loss: 0.9381 | test_acc: 0.6799\n",
      "Epoch: 2 | train_loss: 0.8953 | train_acc: 0.6719 | test_loss: 0.8351 | test_acc: 0.8447\n",
      "Epoch: 3 | train_loss: 0.8227 | train_acc: 0.6914 | test_loss: 0.7777 | test_acc: 0.8352\n",
      "Epoch: 4 | train_loss: 0.7385 | train_acc: 0.7812 | test_loss: 0.7414 | test_acc: 0.8153\n",
      "Epoch: 5 | train_loss: 0.7253 | train_acc: 0.7617 | test_loss: 0.7075 | test_acc: 0.8258\n",
      "total train time took 19.599224099999446 on cuda\n",
      "experiment number: 4\n",
      "number of epochs: 5\n",
      "model: effnetb2\n",
      "data_loader:data_20_percent\n",
      "Created effnetb2 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1603def4b95e42e3925329fe535bf7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9819 | train_acc: 0.5604 | test_loss: 0.7578 | test_acc: 0.8551\n",
      "Epoch: 2 | train_loss: 0.7309 | train_acc: 0.8042 | test_loss: 0.6631 | test_acc: 0.8561\n",
      "Epoch: 3 | train_loss: 0.5988 | train_acc: 0.8521 | test_loss: 0.5708 | test_acc: 0.8958\n",
      "Epoch: 4 | train_loss: 0.4859 | train_acc: 0.9083 | test_loss: 0.5246 | test_acc: 0.8769\n",
      "Epoch: 5 | train_loss: 0.4775 | train_acc: 0.8354 | test_loss: 0.5045 | test_acc: 0.8466\n",
      "total train time took 33.39473379999981 on cuda\n",
      "experiment number: 5\n",
      "number of epochs: 10\n",
      "model: effnetb0\n",
      "data_loader:data_10_percent\n",
      "created effnetb0 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5a67c0317740b79fdd73c4805c32a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0520 | train_acc: 0.4961 | test_loss: 0.8505 | test_acc: 0.6496\n",
      "Epoch: 2 | train_loss: 0.9251 | train_acc: 0.6055 | test_loss: 0.7662 | test_acc: 0.7017\n",
      "Epoch: 3 | train_loss: 0.7707 | train_acc: 0.6992 | test_loss: 0.6602 | test_acc: 0.8759\n",
      "Epoch: 4 | train_loss: 0.7085 | train_acc: 0.7617 | test_loss: 0.6719 | test_acc: 0.8059\n",
      "Epoch: 5 | train_loss: 0.7050 | train_acc: 0.7773 | test_loss: 0.6078 | test_acc: 0.8759\n",
      "Epoch: 6 | train_loss: 0.5833 | train_acc: 0.8008 | test_loss: 0.5891 | test_acc: 0.8674\n",
      "Epoch: 7 | train_loss: 0.5521 | train_acc: 0.9180 | test_loss: 0.5478 | test_acc: 0.8456\n",
      "Epoch: 8 | train_loss: 0.4809 | train_acc: 0.9336 | test_loss: 0.4997 | test_acc: 0.9062\n",
      "Epoch: 9 | train_loss: 0.4577 | train_acc: 0.9336 | test_loss: 0.5595 | test_acc: 0.8258\n",
      "Epoch: 10 | train_loss: 0.5406 | train_acc: 0.7930 | test_loss: 0.5234 | test_acc: 0.8258\n",
      "total train time took 34.748338800000056 on cuda\n",
      "experiment number: 6\n",
      "number of epochs: 10\n",
      "model: effnetb0\n",
      "data_loader:data_20_percent\n",
      "created effnetb0 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb87c7c7324542a3b106dc2c3b3f236d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9588 | train_acc: 0.6125 | test_loss: 0.6506 | test_acc: 0.8759\n",
      "Epoch: 2 | train_loss: 0.6833 | train_acc: 0.8438 | test_loss: 0.5657 | test_acc: 0.9072\n",
      "Epoch: 3 | train_loss: 0.5729 | train_acc: 0.8792 | test_loss: 0.4881 | test_acc: 0.9072\n",
      "Epoch: 4 | train_loss: 0.5320 | train_acc: 0.8271 | test_loss: 0.4978 | test_acc: 0.8570\n",
      "Epoch: 5 | train_loss: 0.4486 | train_acc: 0.8833 | test_loss: 0.4077 | test_acc: 0.9176\n",
      "Epoch: 6 | train_loss: 0.4670 | train_acc: 0.8438 | test_loss: 0.3895 | test_acc: 0.8977\n",
      "Epoch: 7 | train_loss: 0.3625 | train_acc: 0.9062 | test_loss: 0.3452 | test_acc: 0.8968\n",
      "Epoch: 8 | train_loss: 0.3405 | train_acc: 0.9125 | test_loss: 0.3040 | test_acc: 0.9167\n",
      "Epoch: 9 | train_loss: 0.3388 | train_acc: 0.9021 | test_loss: 0.3982 | test_acc: 0.8883\n",
      "Epoch: 10 | train_loss: 0.3613 | train_acc: 0.8521 | test_loss: 0.3127 | test_acc: 0.9271\n",
      "total train time took 58.93437200000062 on cuda\n",
      "experiment number: 7\n",
      "number of epochs: 10\n",
      "model: effnetb2\n",
      "data_loader:data_10_percent\n",
      "Created effnetb2 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fcae03bccc4300949bd4c6a6550fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0914 | train_acc: 0.3867 | test_loss: 0.9381 | test_acc: 0.6799\n",
      "Epoch: 2 | train_loss: 0.8953 | train_acc: 0.6719 | test_loss: 0.8351 | test_acc: 0.8447\n",
      "Epoch: 3 | train_loss: 0.8227 | train_acc: 0.6914 | test_loss: 0.7777 | test_acc: 0.8352\n",
      "Epoch: 4 | train_loss: 0.7385 | train_acc: 0.7812 | test_loss: 0.7414 | test_acc: 0.8153\n",
      "Epoch: 5 | train_loss: 0.7253 | train_acc: 0.7617 | test_loss: 0.7075 | test_acc: 0.8258\n",
      "Epoch: 6 | train_loss: 0.6059 | train_acc: 0.7852 | test_loss: 0.6875 | test_acc: 0.7955\n",
      "Epoch: 7 | train_loss: 0.5351 | train_acc: 0.9141 | test_loss: 0.6133 | test_acc: 0.9062\n",
      "Epoch: 8 | train_loss: 0.5073 | train_acc: 0.9219 | test_loss: 0.6103 | test_acc: 0.8864\n",
      "Epoch: 9 | train_loss: 0.5477 | train_acc: 0.7812 | test_loss: 0.6468 | test_acc: 0.8561\n",
      "Epoch: 10 | train_loss: 0.4857 | train_acc: 0.9336 | test_loss: 0.5311 | test_acc: 0.8864\n",
      "total train time took 38.21960709999985 on cuda\n",
      "experiment number: 8\n",
      "number of epochs: 10\n",
      "model: effnetb2\n",
      "data_loader:data_20_percent\n",
      "Created effnetb2 model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e815200b1da44172a0fe9dff0149e15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9819 | train_acc: 0.5604 | test_loss: 0.7578 | test_acc: 0.8551\n",
      "Epoch: 2 | train_loss: 0.7309 | train_acc: 0.8042 | test_loss: 0.6631 | test_acc: 0.8561\n",
      "Epoch: 3 | train_loss: 0.5988 | train_acc: 0.8521 | test_loss: 0.5708 | test_acc: 0.8958\n",
      "Epoch: 4 | train_loss: 0.4859 | train_acc: 0.9083 | test_loss: 0.5246 | test_acc: 0.8769\n",
      "Epoch: 5 | train_loss: 0.4775 | train_acc: 0.8354 | test_loss: 0.5045 | test_acc: 0.8466\n",
      "Epoch: 6 | train_loss: 0.4247 | train_acc: 0.8750 | test_loss: 0.4793 | test_acc: 0.8466\n",
      "Epoch: 7 | train_loss: 0.3475 | train_acc: 0.9437 | test_loss: 0.4006 | test_acc: 0.9062\n",
      "Epoch: 8 | train_loss: 0.3321 | train_acc: 0.9375 | test_loss: 0.3979 | test_acc: 0.8864\n",
      "Epoch: 9 | train_loss: 0.3371 | train_acc: 0.8938 | test_loss: 0.4662 | test_acc: 0.8665\n",
      "Epoch: 10 | train_loss: 0.3599 | train_acc: 0.8875 | test_loss: 0.3828 | test_acc: 0.8968\n",
      "total train time took 65.82697170000029 on cuda\n",
      "CPU times: total: 19min 55s\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "set_seed()\n",
    "experiment_tracking = 0\n",
    "\n",
    "for epochs in num_epochs:\n",
    "    \n",
    "    for model_name in models:\n",
    "        \n",
    "        for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "            \n",
    "            experiment_tracking += 1\n",
    "            print(f\"experiment number: {experiment_tracking}\\nnumber of epochs: {epochs}\\nmodel: {model_name}\\ndata_loader:{dataloader_name}\")\n",
    "            \n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0()\n",
    "                \n",
    "            else:\n",
    "                model = create_effnetb2()\n",
    "            \n",
    "            model.to(device)\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
    "            \n",
    "            train(model,\n",
    "                  train_dataloader,\n",
    "                  test_dataloader,\n",
    "                  optimizer,\n",
    "                  loss_fn,\n",
    "                  device,\n",
    "                  epochs,\n",
    "                  writer,\n",
    "                  experiment_tracking\n",
    "#                   writer=create_writer(dataloader_name,\n",
    "#                                       model_name,\n",
    "#                                       extra=f\"{epochs}_epochs\")\n",
    "                                \n",
    "            )\n",
    "            \n",
    "            cur_time = dt.now().strftime(\"%b%d-%Y\")\n",
    "            \n",
    "            saved_model_name = f\"{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            assert saved_model_name.endswith(\"pth\") or saved_model_name.endswith(\"th\")\n",
    "            \n",
    "            save_path = Path(\"Model\")/cur_time/saved_model_name\n",
    "            (Path(\"Model\")/cur_time).mkdir(parents=True,exist_ok=True)\n",
    "            \n",
    "            torch.save(obj= model.state_dict(),\n",
    "                      f= save_path)                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15738a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e0b8cc0f2fa42853\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e0b8cc0f2fa42853\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir runs2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92b22ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14560), started 0:00:52 ago. (Use '!kill 14560' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-265f6e6060da8889\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-265f6e6060da8889\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c90db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###in case of tensorboard error\n",
    "# taskkill /im tensorboard.exe /f\n",
    "# del /q %TMP%\\.tensorboard-info\\*\n",
    "# C:\\Users\\pouya\\AppData\\Local\\Temp\\.tensorboard-info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"Model/Sep11-2023/effnetb2_data_20_percent_10_epochs.pth\"\n",
    "\n",
    "best_model = create_effnetb2()\n",
    "\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "####practice\n",
    "\n",
    "# from torchmetrics import ConfusionMatrix\n",
    "# from mlxtend.plotting import plot_confusion_matrix \n",
    "\n",
    "# confmat = ConfusionMatrix(num_classes=len(class_names),task=\"multiclass\")\n",
    "# confmat_tensor = confmat(y_pred,\n",
    "#                         test_data_10_percent.targets)\n",
    "\n",
    "# ax, fig = plot_confusion_matrix(confmat_tensor.numpy(),\n",
    "#                                class_names=class_names,\n",
    "#                                figsize=(15,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " f\"effnetb2: {Path(best_model_path).stat().st_size // (1024*1024)} Mb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pred_and_plot(model,\n",
    "                 img_path,\n",
    "                  class_names,\n",
    "                  img_size = (224, 224),\n",
    "                 transform = None,\n",
    "                 ):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    if transform:\n",
    "        img_transform = transform(img)\n",
    "    else:\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),                             \n",
    "        ])\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        transformed_img = img_transform(img).unsqueeze(0)\n",
    "        targ_img_pred = model(transformed_img.to(device))\n",
    "        \n",
    "    targ_img_prob = torch.softmax(targ_img_pred,dim=1)\n",
    "    targ_img_label = torch.argmax(targ_img_prob,dim=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"pred: {class_names[targ_img_label]}| prob: {targ_img_prob.max()}\")\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_dir_20_percent_path_str = \"data/pizza_steak_sushi_20_percent/test\"\n",
    "\n",
    "test_img_path_list = list(Path(test_dir_20_percent_path_str).glob(\"*/*.jpg\"))\n",
    "test_img_path_samble = random.sample(test_img_path_list, k=3)\n",
    "\n",
    "for path in test_img_path_samble:\n",
    "    pred_and_plot(best_model,\n",
    "                  path,\n",
    "                  class_names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7533f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "custom_img_path = Path(\"data/04-pizza-dad.jpg\")\n",
    "\n",
    "if not custom_img_path.is_file():\n",
    "    with open(custom_img_path,\"wb\") as f:\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        f.write(request.content)        \n",
    "else:\n",
    "    print(f\"skipping download,{custom_img_path} already exist\")\n",
    "    \n",
    "pred_and_plot(best_model,\n",
    "              custom_img_path,\n",
    "              class_names\n",
    "             )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_pred_labels = []\n",
    "    for X, y in test_dataloader_10_percent:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = best_model(X)\n",
    "        y_pred_prob = torch.softmax(y_pred,dim=1)\n",
    "        y_pred_label = torch.argmax(y_pred_prob,dim=1)\n",
    "        y_pred_labels.append(y_pred_label.cpu())\n",
    "        \n",
    "    torch.cat(y_pred_labels)\n",
    "        \n",
    "pred_labels_tensor =  torch.cat(y_pred_labels)      \n",
    "pred_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2043d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_labels = [y for X,y in test_dataloader_10_percent]\n",
    "truth_labels = torch.cat(truth_labels)\n",
    "truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b102dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(test_data_10_percent.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac66c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "###practice\n",
    "\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix \n",
    "\n",
    "confmat = ConfusionMatrix(num_classes=len(class_names),task=\"multiclass\")\n",
    "confmat_tensor = confmat(preds=pred_labels_tensor,\n",
    "                        target=truth_labels)   ##or use torch.tensor(test_data_10_percent.targets)\n",
    "\n",
    "ax, fig = plot_confusion_matrix(confmat_tensor.numpy(),\n",
    "                               class_names=class_names,\n",
    "                               figsize=(10,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fa5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
